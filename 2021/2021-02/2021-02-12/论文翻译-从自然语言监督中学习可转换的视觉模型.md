# 论文翻译-从自然语言监督中学习可转换的视觉模型

Learning Transferable Visual Models From Natural Language Supervision

## 摘要
最先进的计算机视觉系统训练预测一组固定的预定对象类别。这种受限的超级视觉形式限制了它们的通用性和可用性，因为需要额外的标记数据来指定任何其他视觉概念。直接从原始文本中学习图像是一种很有前途的选择，它利用了更广泛的监督来源。我们证明，在从互联网上收集的4亿对（图像、文本）数据集上，预测哪个字幕与哪个图像匹配的简单预训练任务是一种有效且可扩展的方法，可以从零开始学习SOTA图像表示。在预训练之后，自然语言被用来引用学习到的视觉概念（或描述新的概念），从而实现模型到下游任务的零镜头传输。我们通过在30多个不同的现有计算机视觉数据集上进行基准测试来研究这种方法的性能，这些数据集跨越了OCR、视频中的动作识别、地理定位和许多类型的细粒度对象分类等任务。该模型可以很容易地转移到大多数任务中，并且通常与完全监督的基线竞争，而不需要任何特定于数据集的训练。例如，我们在ImageNet zero shot上匹配原始ResNet-50的精度，而不需要使用128万个训练示例中的任何一个。

详情请看：[Learning Transferable Visual Models From Natural Language Supervision](../../../论文/papers/Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision.md)
