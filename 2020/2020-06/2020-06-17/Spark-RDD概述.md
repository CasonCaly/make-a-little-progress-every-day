# Spark-RDD概述

  - [什么是 RDD](#%E4%BB%80%E4%B9%88%E6%98%AF-rdd)
  - [RDD 的属性](#rdd-%E7%9A%84%E5%B1%9E%E6%80%A7)
  - [RDD 的特点](#rdd-%E7%9A%84%E7%89%B9%E7%82%B9)
    - [分区](#%E5%88%86%E5%8C%BA)
    - [只读](#%E5%8F%AA%E8%AF%BB)
    - [依赖](#%E4%BE%9D%E8%B5%96)
    - [缓存](#%E7%BC%93%E5%AD%98)
    - [CheckPoint](#checkpoint)

## 什么是 RDD
RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据（计算）抽象。代码中是一个抽象类，它代表一个不可变、可分区、里面的元素可并行计算的集合。

## RDD 的属性
* 一组分区（Partition），即数据集的基本组成单位
* 一个计算每个分区的函数
* RDD 之间的依赖关系
* 一个 Partitioner，即 RDD 的分片函数
* 一个列表，存储存取每个 Partition 的优先位置（preferred location）

## RDD 的特点
RDD 表示只读的分区的数据集，对 RDD 进行改动，只能通过 RDD 的转换操作，由一个 RDD 得到一个新的 RDD，新的 RDD 包含了从其他 RDD 衍生所必需的信息。RDDs 之间存在依赖，RDD 的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化 RDD 来切断血缘关系。

### 分区
RDD 逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个 compute 函数得到每个分区的数据。如果 RDD 是通过已有的文件系统构建，则 compute 函数是读取指定文件系统中的数据，如果 RDD 是通过其他 RDD 转换而来，则 compute 函数是执行转换逻辑将其他 RDD 的数据进行转换。

### 只读
RDD 是只读的，要想改变 RDD 中的数据，只能在现有的 RDD 基础上创建新的 RDD。由一个 RDD 转换到另一个 RDD，可以通过丰富的操作算子实现，不再像 MapReduce 那样只能写 map 和 reduce 了，如下图所示

![丰富的操作算子](https://cdn.jsdelivr.net/gh/ylsislove/image-home/test/20200617195044.png)

RDD 的操作算子包括两类，一类叫做 transformations，它是用来将 RDD 进行转化，构建 RDD 的血缘关系；另一类叫做 actions，它是用来触发 RDD 的计算，得到 RDD 的相关计算结果或者将 RDD 保存的文件系统中。

### 依赖
RDDs 通过操作算子进行转换，转换得到的新 RDD 包含了从其他 RDDs 衍生所必需的信息，RDDs 之间维护着这种血缘关系，也称之为依赖。如上图所示，依赖包括两种，一种是窄依赖，RDDs 之间分区是一一对应的，另一种是宽依赖，下游 RDD 的每个分区与上游 RDD （也称之为父 RDD）的每个分区都有关，是多对多的关系。

### 缓存
如果在应用程序中多次使用同一个 RDD，可以将该 RDD 缓存起来，该 RDD 只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该 RDD 的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。

### CheckPoint
虽然 RDD 的血缘关系天然地可以实现容错，当 RDD 的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs 之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD 支持 checkpoint 将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为 checkpoint 后的 RDD 不需要知道它的父 RDDs 了，它可以从 checkpoint 处拿到数据。
